{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUACIÓN DE ETIQUETADORES MORFOSINTÁCTICOS PARA EL ESPAÑOL\n",
    "### Lingüística Computacional 2021/2022\n",
    "\n",
    "#### Autor: Luis Cardoza Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAREA 1\n",
    "\n",
    "Evaluación del etiquetador ‘hmm’ sobre el corpus ‘cess-esp’ utilizando el juego de categorías completo y reducido.\n",
    "\n",
    "Utilizando el etiquetador hmm basado en modelos de Markov, se realizará una validación cruzada sobre 10 particiones del corpus. Barajar el corpus antes de realizar las particiones. Presentar los resultados en forma de tabla y gráficamente, incluyendo los intervalos de confianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/crdzbird/Documents/MasterInteligenciaArtificialUPV/LC/proyecto_final/Proyecto.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/crdzbird/Documents/MasterInteligenciaArtificialUPV/LC/proyecto_final/Proyecto.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crdzbird/Documents/MasterInteligenciaArtificialUPV/LC/proyecto_final/Proyecto.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mcess_esp\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crdzbird/Documents/MasterInteligenciaArtificialUPV/LC/proyecto_final/Proyecto.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m cess_esp\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.tag import hmm\n",
    "from random import shuffle\n",
    "\n",
    "corpus_sentences = [ x for x in cess_esp.tagged_sents()]\n",
    "\n",
    "shuffle(corpus_sentences)\n",
    "\n",
    "procesed_sentences = []\n",
    "\n",
    "for sentence in corpus_sentences:\n",
    "\n",
    "    words = []\n",
    "\n",
    "    for word in sentence:\n",
    "\n",
    "        if (\"*0*\" not in word[0]):\n",
    "\n",
    "            lenght = 2\n",
    "\n",
    "            if (word[1][0].lower() == \"v\" or word[1][0].lower() == \"f\"):\n",
    "                lenght = 3\n",
    "\n",
    "            lenght = min(lenght, len(word[1]))\n",
    "\n",
    "            words.append((word[0], word[1][0:lenght]))\n",
    "\n",
    "    procesed_sentences.append(words)\n",
    "\n",
    "corpus_sentences_reduced = procesed_sentences\n",
    "\n",
    "print(\"\\nDataset preview: \" + str(corpus_sentences[0][0:10]))\n",
    "print(\"\\nDataset reduced preview: \" + str(corpus_sentences_reduced[0][0:10]))\n",
    "\n",
    "print(f\"\\nOriginal dataset size: {len(corpus_sentences_reduced)}\")\n",
    "print(f\"\\nReduced dataset size: {len(corpus_sentences)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
